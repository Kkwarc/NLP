{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kwarc\\PycharmProjects\\NLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\kwarc\\PycharmProjects\\NLP\\.venv\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\kwarc\\PycharmProjects\\NLP\\.venv\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "from models import MLP\n",
    "from get_embedded_data import get_data_tokenizer_MLP, split_data, MAPPING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, ElectraModel, RobertaModel\n",
    "from transformers import BertTokenizer, ElectraTokenizer, RobertaTokenizer\n",
    "\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "robert_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "robert_model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "electra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "electra_model = ElectraModel.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_epoch, network, train_dataloader, test_dataloader, optimizer, criterion):\n",
    "    train_data_df = []\n",
    "\n",
    "    network.train()\n",
    "    best_network = network\n",
    "    best_accuracy = 0\n",
    "    dict_for_stat = {\n",
    "        0: [0,0,0],\n",
    "        1: [0,0,0],\n",
    "        2: [0,0,0],\n",
    "        3: [0,0,0],\n",
    "        4: [0,0,0],\n",
    "        5: [0,0,0],\n",
    "        6: [0,0,0],\n",
    "        7: [0,0,0],\n",
    "        8: [0,0,0]\n",
    "    }\n",
    "    for epoch in range(max_epoch):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = torch.flatten(inputs.double(), 1)\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            total += labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        for i, val_data in enumerate(test_dataloader, 0):\n",
    "            val_inputs, val_labels = val_data\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "\n",
    "            val_inputs = torch.flatten(val_inputs.double(), 1)\n",
    "            val_outputs = network(val_inputs)\n",
    "\n",
    "            val_total += val_labels.size(0)\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "        if round(100 * val_correct / val_total, 3) > best_accuracy:\n",
    "            best_accuracy = round(100 * val_correct / val_total, 3)\n",
    "            best_network = copy.deepcopy(network)\n",
    "\n",
    "        train_loss = running_loss / 2000\n",
    "        train_accuracy = round(100 * correct / total, 3)\n",
    "        val_accuracy = round(100 * val_correct / val_total, 3)\n",
    "        train_data_df.append([epoch+1, max_epoch, round(train_loss, 3), train_accuracy, val_accuracy])\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    start_time = time.time()\n",
    "    for i, data in enumerate(test_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        inputs = torch.flatten(inputs.double(), 1)\n",
    "        outputs = best_network(inputs)\n",
    "\n",
    "\n",
    "        total += labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        for pr, lab in zip(predicted, labels):\n",
    "            pr, lab = pr.item(), lab.item()\n",
    "            if pr == lab:\n",
    "                # TP\n",
    "                dict_for_stat[pr][0] += 1\n",
    "                continue\n",
    "            # FN\n",
    "            dict_for_stat[lab][1] += 1\n",
    "            # FP\n",
    "            dict_for_stat[pr][2] += 1\n",
    "    pred_time = start_time - time.time()\n",
    "\n",
    "    pr_rec_f1 = {}\n",
    "    for key in dict_for_stat.keys():\n",
    "        tp, fn, fp = dict_for_stat[key]\n",
    "        precision = -1 if tp+fp == 0 else tp/(tp+fp)\n",
    "        recall = -1 if tp+fn == 0 else tp/(tp+fn)\n",
    "        f1_score = -1 if tp+fn+fp == 0 else 2*tp/(2*tp+fn+fp)\n",
    "        pr_rec_f1[key] = [precision, recall, f1_score]\n",
    "\n",
    "    final_accuracy = round(100 * correct / total, 3)\n",
    "\n",
    "    return best_network, final_accuracy, best_accuracy, all_labels, all_predictions, pr_rec_f1, pred_time, pd.DataFrame(train_data_df, columns=[\"Epoch\", \"Max Epoch\", \"loss\", \"train data accuracy\", \"test data accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aristotle' 'freud' 'hegel' 'kant' 'nietzsche' 'plato' 'sartre'\n",
      " 'schopenhauer' 'spinoza']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kwarc\\PycharmProjects\\NLP\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "models_list = [(\"bert\", bert_tokenizer, bert_model), (\"roberta\", robert_tokenizer, robert_model), (\"electra\", electra_tokenizer, electra_model)]\n",
    "labels_to_delete_list = [[], ['spinoza', \"hegel\", \"plato\"], ['aristotle', 'freud', 'kant', 'nietzsche', 'sartre', 'schopenhauer']]\n",
    "weigth_list = [None, torch.tensor([1,1,2,3,1.33,1.33,5,1,2.5],dtype=torch.double).to(device)]\n",
    "\n",
    "full_data = []\n",
    "for labels_to_delete in labels_to_delete_list:\n",
    "    X_train, X_test, y_train, y_test = split_data(\"data_set.csv\", \"author\", \"quote\", test_size=0.2, separator=\"@\", mapping=MAPPING,\n",
    "                                                labels_to_delete=labels_to_delete)\n",
    "    for name, tokenizer, model in models_list:\n",
    "        train_dataloader, shape = get_data_tokenizer_MLP(batch=20*5, words=X_train, labels=y_train, device=device, tokenizer=tokenizer, model=model)\n",
    "        test_dataloader, shape = get_data_tokenizer_MLP(batch=1, words=X_test, labels=y_test, device=device, tokenizer=tokenizer, model=model)\n",
    "        for weigths in weigth_list:\n",
    "            network = MLP(shape*125, 125*2, 125, 9, dropout=0).to(device)\n",
    "            criterion = nn.CrossEntropyLoss(weight=weigths)\n",
    "            optimizer = optim.Adam(network.parameters())\n",
    "            best_network, test_acc, best_accuracy, all_labels, all_predictions, pr_rec_f1, pred_time, train_data = train(max_epoch=20, network=network, train_dataloader=train_dataloader, test_dataloader=test_dataloader, optimizer=optimizer, criterion=criterion)\n",
    "            weigths = [] if weigths is None else [1,1,2,3,1.33,1.33,5,1,2.5]\n",
    "            torch.save(best_network, f\"train_models/{name}_{labels_to_delete}_{weigths}.pt\")\n",
    "            train_data.to_csv(f\"train_data/{name}_{labels_to_delete}_{weigths}.csv\")\n",
    "            full_data.append([name, labels_to_delete, weigths, test_acc, best_accuracy, all_labels, all_predictions, pr_rec_f1, pred_time])\n",
    "full_data_df = pd.DataFrame(full_data, columns=[\"name\", \"labels_to_delete\", \"weigths\", \"test_acc\", \"best_accuracy\", \"all_labels\", \"all_predictions\", \"pr_rec_f1\", \"pred_time\"])\n",
    "full_data_df.to_csv(\"train_data/full_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix([[MAPPING[x] for x in all_labels], [MAPPING[x] for x in all_predictions])\n",
    "\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
